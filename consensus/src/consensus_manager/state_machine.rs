use std::{
    sync::{
        Arc,
        atomic::{AtomicBool, Ordering},
    },
    time::{Duration, Instant},
};

use anyhow::Result;
use rtrb::{Consumer, Producer};

use crate::{
    consensus::ConsensusMessage,
    consensus_manager::{events::ViewProgressEvent, view_manager::ViewProgressManager},
    crypto::aggregated::{BlsSecretKey, PeerId},
    state::{block::Block, notarizations::Vote, transaction::Transaction},
};

/// Maximum number of attempts to broadcast a consensus message, in case the ring buffer is full
const MAX_BROADCAST_ATTEMPTS: usize = 10;

/// Consensus state machine that orchestrates the Minimmit protocol
pub struct ConsensusStateMachine<const N: usize, const F: usize, const M_SIZE: usize> {
    /// The view progress manager that drives consensus logic
    view_manager: ViewProgressManager<N, F, M_SIZE>,

    /// Secret key for signing messages
    secret_key: BlsSecretKey,

    /// Channel for receiving consensus messages from the network
    message_consumer: Consumer<ConsensusMessage<N, F, M_SIZE>>,

    /// Channel for broadcasting consensus messages to the network
    broadcast_producer: Producer<ConsensusMessage<N, F, M_SIZE>>,

    /// Channel for receiving transactions to include in blocks
    transaction_consumer: Consumer<Transaction>,

    /// Tick interval for checking timeouts and triggering periodic actions
    tick_interval: Duration,

    /// Signal to shutdown the state machine
    /// This is used to signal to the state machine that it should shutdown gracefully.
    shutdown_signal: Arc<AtomicBool>,

    /// Logger for logging events
    logger: slog::Logger,
}

impl<const N: usize, const F: usize, const M_SIZE: usize> ConsensusStateMachine<N, F, M_SIZE> {
    #[allow(clippy::too_many_arguments)]
    pub fn new(
        view_manager: ViewProgressManager<N, F, M_SIZE>,
        secret_key: BlsSecretKey,
        message_consumer: Consumer<ConsensusMessage<N, F, M_SIZE>>,
        broadcast_producer: Producer<ConsensusMessage<N, F, M_SIZE>>,
        transaction_consumer: Consumer<Transaction>,
        tick_interval: Duration,
        shutdown_signal: Arc<AtomicBool>,
        logger: slog::Logger,
    ) -> Result<Self> {
        Ok(Self {
            view_manager,
            secret_key,
            message_consumer,
            broadcast_producer,
            transaction_consumer,
            tick_interval,
            shutdown_signal,
            logger,
        })
    }

    /// Continuously processes messages from the network and transactions from the application.
    pub fn run(&mut self) -> Result<()> {
        slog::info!(
            self.logger,
            "Consensus state machine started for replica {}",
            self.view_manager.replica_id()
        );

        let mut last_tick = Instant::now();

        while !self.shutdown_signal.load(Ordering::Relaxed) {
            // Process all available consensus messages from the network
            while let Ok(message) = self.message_consumer.pop() {
                if let Err(e) = self.handle_consensus_message(message) {
                    slog::error!(self.logger, "Error handling consensus message: {}", e);
                }
            }

            // Process all available transactions from the application
            while let Ok(transaction) = self.transaction_consumer.pop() {
                self.view_manager.add_transaction(transaction);
            }

            // Periodic tick
            if last_tick.elapsed() >= self.tick_interval {
                if let Err(e) = self.view_manager.tick() {
                    slog::error!(self.logger, "Error ticking view manager: {}", e);
                }
                last_tick = Instant::now();
            }

            // Spin loop to prevent busy waiting
            std::hint::spin_loop();
        }

        slog::info!(
            self.logger,
            "Consensus state machine stopped for replica {}",
            self.view_manager.replica_id()
        );

        Ok(())
    }

    /// Stop the state machine gracefully
    pub fn stop(&mut self) -> Result<()> {
        self.shutdown_signal.store(true, Ordering::Relaxed);
        self.view_manager.shutdown()?;
        slog::info!(
            self.logger,
            "Consensus state machine stopped for replica {}",
            self.view_manager.replica_id()
        );
        Ok(())
    }

    /// Handles any incoming consensus messages
    fn handle_consensus_message(&mut self, message: ConsensusMessage<N, F, M_SIZE>) -> Result<()> {
        let event = self.view_manager.process_consensus_msg(message)?;
        self.handle_event(event)
    }

    /// Handles periodic tick
    pub fn handle_tick(&mut self) -> Result<()> {
        let event = self.view_manager.tick()?;
        self.handle_event(event)
    }

    /// Handles a view progress event by taking appropriate action
    fn handle_event(&mut self, event: ViewProgressEvent<N, F, M_SIZE>) -> Result<()> {
        match event {
            ViewProgressEvent::NoOp => {
                // Nothing to do
                Ok(())
            }
            ViewProgressEvent::Await => {
                // Waiting for messages
                Ok(())
            }
            ViewProgressEvent::ShouldProposeBlock {
                view,
                parent_block_hash,
            } => self.propose_block(view, parent_block_hash),
            ViewProgressEvent::ShouldVote { view, block_hash } => {
                self.vote_for_block(view, block_hash)
            }
            ViewProgressEvent::ShouldMNotarize {
                view,
                block_hash,
                should_forward_m_notarization,
            } => self.create_and_broadcast_m_notarization(
                view,
                block_hash,
                should_forward_m_notarization,
            ),
            ViewProgressEvent::ShouldFinalize { view, block_hash } => {
                self.finalize_view(view, block_hash)
            }
            ViewProgressEvent::ShouldNullify { view } => self.nullify_view(view),
            ViewProgressEvent::ShouldBroadcastNullification { view } => {
                self.broadcast_nullification(view)
            }
            ViewProgressEvent::ShouldVoteAndMNotarize {
                view,
                block_hash,
                should_forward_m_notarization,
            } => {
                self.vote_for_block(view, block_hash)?;
                self.create_and_broadcast_m_notarization(
                    view,
                    block_hash,
                    should_forward_m_notarization,
                )
            }
            ViewProgressEvent::ShouldVoteAndFinalize { view, block_hash } => {
                self.vote_for_block(view, block_hash)?;
                self.finalize_view(view, block_hash)
            }
            ViewProgressEvent::ProgressToNextView {
                new_view,
                leader,
                notarized_block_hash,
                should_forward_m_notarization,
            } => {
                slog::info!(
                    self.logger,
                    "Progressed to view {} with M-notarization (block: {:?}, leader: {:?})",
                    new_view,
                    notarized_block_hash,
                    leader
                );
                self.create_and_broadcast_m_notarization(
                    new_view - 1, /* NOTE: The M-notarization is for the previous view (new_view
                                   * - 1) */
                    notarized_block_hash,
                    should_forward_m_notarization,
                )?;
                self.progress_to_next_view(new_view, leader, notarized_block_hash)
            }
            ViewProgressEvent::ShouldVoteAndProgressToNextView {
                old_view,
                block_hash,
                new_view,
                leader,
                should_forward_m_notarization,
            } => {
                self.vote_for_block(old_view, block_hash)?;
                self.create_and_broadcast_m_notarization(
                    old_view, // NOTE: The M-notarization is for the previous view (new_view - 1)
                    block_hash,
                    should_forward_m_notarization,
                )?;
                self.progress_to_next_view(new_view, leader, block_hash)?;
                Ok(())
            }
            ViewProgressEvent::ProgressToNextViewOnNullification {
                new_view,
                leader,
                parent_block_hash,
                should_broadcast_nullification,
            } => {
                if should_broadcast_nullification {
                    self.broadcast_nullification(new_view - 1)?; // NOTE: The nullification is for the previous view (new_view - 1)
                }
                self.progress_to_next_view(new_view, leader, parent_block_hash)?;
                Ok(())
            }
            ViewProgressEvent::ShouldUpdateView { new_view, leader } => {
                slog::warn!(
                    self.logger,
                    "Behind! Should update to view {} (leader: {:?})",
                    new_view,
                    leader
                );
                // TODO: In a full implementation, we'd request missing state here
                slog::info!(self.logger, "TODO: Request missing state here");
                Ok(())
            }
            ViewProgressEvent::ShouldNullifyView { view } => self.nullify_view(view),
            ViewProgressEvent::BroadcastConsensusMessage { message } => {
                self.broadcast_consensus_message(*message)
            }
        }
    }

    /// Proposes a block, as a leader, for the current view
    fn propose_block(&mut self, view: u64, parent_block_hash: [u8; blake3::OUT_LEN]) -> Result<()> {
        slog::debug!(
            self.logger,
            "Proposing block for view {view} with parent block hash {parent_block_hash:?}"
        );

        // Get pending transactions
        let transactions = self.view_manager.take_pending_transactions();

        let timestamp = std::time::SystemTime::now()
            .duration_since(std::time::UNIX_EPOCH)
            .unwrap()
            .as_secs();
        let block_hash = compute_block_hash(parent_block_hash, &transactions, timestamp, view);
        let leader_signature = self.secret_key.sign(&block_hash);

        // Create a new block
        let block = Block::new(
            view,
            self.view_manager.replica_id(), /* It is the current leader replica that is
                                             * proposing the block */
            parent_block_hash,
            transactions,
            timestamp,
            leader_signature,
            false,
            view,
        );

        // Broadcast the block proposal to the network layer (to be received by other replicas)
        self.broadcast_consensus_message(ConsensusMessage::BlockProposal(block))?;

        // Mark the block as proposed
        self.view_manager.mark_proposed(view)?;

        // Mark the block as voted
        self.view_manager.mark_voted(view)?;

        Ok(())
    }

    /// Vote for a block
    fn vote_for_block(&mut self, view: u64, block_hash: [u8; blake3::OUT_LEN]) -> Result<()> {
        slog::debug!(
            self.logger,
            "Voting for block for view {view} with block hash {block_hash:?}"
        );

        // Get leader for this view
        let leader_id = self.view_manager.leader_for_view(view)?;

        // Sign the vote
        let vote_signature = self.secret_key.sign(&block_hash);

        // Create a new vote
        let vote = Vote::new(
            view,
            block_hash,
            vote_signature.clone(),
            self.view_manager.replica_id(),
            leader_id,
        );

        // Broadcast the vote to the network layer (to be received by other replicas)
        self.broadcast_consensus_message(ConsensusMessage::Vote(vote))?;

        // Mark the vote as cast
        self.view_manager.mark_voted(view)?;

        // Add the vote to the view context
        self.view_manager.add_own_vote(view, vote_signature)?;

        Ok(())
    }

    /// Create and broadcast an M-notarization for a block
    fn create_and_broadcast_m_notarization(
        &mut self,
        view: u64,
        block_hash: [u8; blake3::OUT_LEN],
        should_forward: bool,
    ) -> Result<()> {
        slog::debug!(
            self.logger,
            "Creating and broadcasting M-notarization for view {view} with block hash {block_hash:?}"
        );

        if should_forward {
            // Get the M-notarization for this view
            let m_notarization = self.view_manager.get_m_notarization(view)?;

            self.broadcast_consensus_message(ConsensusMessage::MNotarization(m_notarization))?;
        }

        Ok(())
    }

    /// Finalize a view with a L-notarization (n-f votes for the view block)
    fn finalize_view(&mut self, view: u64, block_hash: [u8; blake3::OUT_LEN]) -> Result<()> {
        slog::debug!(
            self.logger,
            "Finalizing view {view} with block hash {block_hash:?}"
        );

        self.view_manager.finalize_view(view)?;

        Ok(())
    }

    /// Nullify a view
    fn nullify_view(&mut self, view: u64) -> Result<()> {
        slog::debug!(self.logger, "Nullifying view {view}");

        // Get view context to create nullify message
        let view_ctx = self.view_manager.view_context_mut(view)?;

        let nullify = if view_ctx.has_voted {
            // NOTE: After voting, the current replica must have conflicting evidence,
            // so in this case, the view is under Byzantine behavior.
            view_ctx.create_nullify_for_byzantine(&self.secret_key)?
        } else {
            // NOTE: If the current replica attempts to nullify a message before voting, it could be
            // timeout OR Byzantine We can't distinguish here, so we check if there's
            // conflicting evidence
            let conflicting_count = view_ctx.nullify_messages.len() + view_ctx.num_invalid_votes;

            if conflicting_count > 2 * F {
                // Byzantine behavior detected before voting
                view_ctx.create_nullify_for_byzantine(&self.secret_key)?
            } else {
                // Timeout
                view_ctx.create_nullify_for_timeout(&self.secret_key)?
            }
        };

        // Broadcast the nullify message to the network layer (to be received by other replicas)
        self.broadcast_consensus_message(ConsensusMessage::Nullify(nullify))?;

        // Mark the nullification as cast
        self.view_manager.mark_nullified(view)?;

        Ok(())
    }

    /// Broadcast a nullification for a view
    fn broadcast_nullification(&mut self, view: u64) -> Result<()> {
        slog::debug!(self.logger, "Broadcasting nullification for view {view}");

        let nullification = self.view_manager.get_nullification(view)?;

        self.broadcast_consensus_message(ConsensusMessage::Nullification(nullification))?;

        Ok(())
    }

    /// Progress to the next view after M-notarization, or nullification
    fn progress_to_next_view(
        &mut self,
        view: u64,
        leader: PeerId,
        notarized_block_hash: [u8; blake3::OUT_LEN],
    ) -> Result<()> {
        slog::debug!(
            self.logger,
            "Progressing to next view {view} with leader {leader} and notarized block hash {notarized_block_hash:?}"
        );

        // Check if the current replica is the leader for the next view
        let replica_id = self.view_manager.replica_id();

        if replica_id == leader {
            // This replica is the leader for the next view, so it should propose a block
            let parent_block_hash = notarized_block_hash;
            self.propose_block(view, parent_block_hash)?;
        }

        Ok(())
    }

    /// Helper to broadcast a consensus message with retry on full buffer
    fn broadcast_consensus_message(
        &mut self,
        message: ConsensusMessage<N, F, M_SIZE>,
    ) -> Result<()> {
        for attempt in 0..MAX_BROADCAST_ATTEMPTS {
            match self.broadcast_producer.push(message.clone()) {
                Ok(_) => {
                    return Ok(());
                }
                Err(rtrb::PushError::Full(_)) => {
                    slog::warn!(
                        self.logger,
                        "Ring buffer is full, retrying, on attempt {} of {MAX_BROADCAST_ATTEMPTS}...",
                        attempt + 1,
                    );
                    std::thread::sleep(Duration::from_millis(1));
                }
            }
        }
        Err(anyhow::anyhow!(
            "Failed to broadcast consensus message after {} attempts",
            MAX_BROADCAST_ATTEMPTS
        ))
    }

    /// Gracefully shuts down the Minimmit consensus state machine
    pub fn shutdown(&mut self) -> Result<()> {
        self.shutdown_signal.store(true, Ordering::Relaxed);
        self.view_manager.shutdown()?;
        slog::info!(
            self.logger,
            "Consensus state machine stopped for replica {}",
            self.view_manager.replica_id()
        );
        Ok(())
    }
}

/// Builder for creating a [`ConsensusStateMachine`] instance
pub struct ConsensusStateMachineBuilder<const N: usize, const F: usize, const M_SIZE: usize> {
    view_manager: Option<ViewProgressManager<N, F, M_SIZE>>,
    secret_key: Option<BlsSecretKey>,
    tick_interval: Option<Duration>,
    shutdown_signal: Option<Arc<AtomicBool>>,
    logger: Option<slog::Logger>,
    message_consumer: Option<Consumer<ConsensusMessage<N, F, M_SIZE>>>,
    broadcast_producer: Option<Producer<ConsensusMessage<N, F, M_SIZE>>>,
    transaction_consumer: Option<Consumer<Transaction>>,
}

impl<const N: usize, const F: usize, const M_SIZE: usize> Default
    for ConsensusStateMachineBuilder<N, F, M_SIZE>
{
    fn default() -> Self {
        Self::new()
    }
}

impl<const N: usize, const F: usize, const M_SIZE: usize>
    ConsensusStateMachineBuilder<N, F, M_SIZE>
{
    pub fn new() -> Self {
        Self {
            view_manager: None,
            secret_key: None,
            tick_interval: None,
            shutdown_signal: None,
            logger: None,
            message_consumer: None,
            broadcast_producer: None,
            transaction_consumer: None,
        }
    }

    pub fn with_view_manager(mut self, view_manager: ViewProgressManager<N, F, M_SIZE>) -> Self {
        self.view_manager = Some(view_manager);
        self
    }

    pub fn with_secret_key(mut self, secret_key: BlsSecretKey) -> Self {
        self.secret_key = Some(secret_key);
        self
    }

    pub fn with_tick_interval(mut self, interval: Duration) -> Self {
        self.tick_interval = Some(interval);
        self
    }

    pub fn with_shutdown_signal(mut self, shutdown_signal: Arc<AtomicBool>) -> Self {
        self.shutdown_signal = Some(shutdown_signal);
        self
    }

    pub fn with_logger(mut self, logger: slog::Logger) -> Self {
        self.logger = Some(logger);
        self
    }

    pub fn with_message_consumer(
        mut self,
        message_consumer: Consumer<ConsensusMessage<N, F, M_SIZE>>,
    ) -> Self {
        self.message_consumer = Some(message_consumer);
        self
    }

    pub fn with_broadcast_producer(
        mut self,
        broadcast_producer: Producer<ConsensusMessage<N, F, M_SIZE>>,
    ) -> Self {
        self.broadcast_producer = Some(broadcast_producer);
        self
    }

    pub fn with_transaction_consumer(
        mut self,
        transaction_consumer: Consumer<Transaction>,
    ) -> Self {
        self.transaction_consumer = Some(transaction_consumer);
        self
    }

    pub fn build(self) -> Result<ConsensusStateMachine<N, F, M_SIZE>> {
        ConsensusStateMachine::new(
            self.view_manager
                .ok_or_else(|| anyhow::anyhow!("ViewProgressManager not set"))?,
            self.secret_key
                .ok_or_else(|| anyhow::anyhow!("SecretKey not set"))?,
            self.message_consumer
                .ok_or_else(|| anyhow::anyhow!("Message consumer not set"))?,
            self.broadcast_producer
                .ok_or_else(|| anyhow::anyhow!("Broadcast producer not set"))?,
            self.transaction_consumer
                .ok_or_else(|| anyhow::anyhow!("Transaction consumer not set"))?,
            self.tick_interval
                .ok_or_else(|| anyhow::anyhow!("Tick interval not set"))?,
            self.shutdown_signal
                .ok_or_else(|| anyhow::anyhow!("Shutdown signal not set"))?,
            self.logger
                .ok_or_else(|| anyhow::anyhow!("Logger not set"))?,
        )
    }
}

fn compute_block_hash(
    parent_block_hash: [u8; blake3::OUT_LEN],
    txs: &[Transaction],
    timestamp: u64,
    view: u64,
) -> [u8; blake3::OUT_LEN] {
    let mut hasher = blake3::Hasher::new();
    hasher.update(&parent_block_hash);
    hasher.update(
        &txs.iter()
            .enumerate()
            .map(|(i, t)| {
                let mut hasher = blake3::Hasher::new();
                hasher.update(&i.to_le_bytes());
                hasher.update(&t.tx_hash);
                hasher.finalize().into()
            })
            .collect::<Vec<[u8; blake3::OUT_LEN]>>()
            .concat(),
    );
    hasher.update(&timestamp.to_le_bytes());
    hasher.update(&view.to_le_bytes());
    hasher.finalize().into()
}

#[cfg(test)]
mod tests {
    use super::*;
    use crate::{
        consensus::ConsensusMessage,
        consensus_manager::{
            config::{ConsensusConfig, Network},
            leader_manager::{LeaderSelectionStrategy, RoundRobinLeaderManager},
            view_manager::ViewProgressManager,
        },
        crypto::aggregated::BlsSecretKey,
        state::{peer::PeerSet, transaction::Transaction},
        storage::store::ConsensusStore,
    };
    use ark_serialize::CanonicalSerialize;
    use rand::thread_rng;
    use rtrb::RingBuffer;
    use std::{
        collections::HashMap,
        sync::{
            Arc,
            atomic::{AtomicBool, Ordering},
        },
        time::Duration,
    };
    use tempfile::tempdir;

    // Test constants matching Minimmit assumptions: n >= 5f + 1
    const N: usize = 6; // 6 processors
    const F: usize = 1; // 1 Byzantine fault
    const M_SIZE: usize = 3; // M-notarization threshold: 2f + 1 = 3

    struct TestSetup {
        state_machines: Vec<ConsensusStateMachine<N, F, M_SIZE>>,
        peer_set: PeerSet,
        broadcast_consumers: Vec<Consumer<ConsensusMessage<N, F, M_SIZE>>>,
        shutdown_signals: Vec<Arc<AtomicBool>>,
    }

    fn create_test_peer_setup(
        size: usize,
    ) -> (
        PeerSet,
        HashMap<crate::crypto::aggregated::PeerId, BlsSecretKey>,
    ) {
        let mut rng = thread_rng();
        let mut public_keys = vec![];
        let mut peer_id_to_secret_key = HashMap::new();

        for _ in 0..size {
            let sk = BlsSecretKey::generate(&mut rng);
            let pk = sk.public_key();
            let peer_id = pk.to_peer_id();
            peer_id_to_secret_key.insert(peer_id, sk);
            public_keys.push(pk);
        }

        (PeerSet::new(public_keys), peer_id_to_secret_key)
    }

    fn create_test_transaction(nonce: u64) -> Transaction {
        let mut rng = thread_rng();
        let sk = BlsSecretKey::generate(&mut rng);
        let pk = sk.public_key();
        let mut tx_data = Vec::new();
        tx_data.extend_from_slice(&nonce.to_le_bytes());
        let tx_hash: [u8; blake3::OUT_LEN] = blake3::hash(&tx_data).into();
        let sig = sk.sign(&tx_hash);
        Transaction::new(pk, [7u8; 32], 42, nonce, 1_000, 3, tx_hash, sig)
    }

    fn create_test_config(n: usize, f: usize, peer_strs: Vec<String>) -> ConsensusConfig {
        ConsensusConfig {
            n,
            f,
            view_timeout: Duration::from_secs(10),
            leader_manager: LeaderSelectionStrategy::RoundRobin,
            network: Network::Local,
            peers: peer_strs,
        }
    }

    fn create_test_setup() -> TestSetup {
        let (peer_set, peer_id_to_secret_key) = create_test_peer_setup(N);
        let secret_keys: Vec<_> = peer_set
            .sorted_peer_ids
            .iter()
            .map(|id| peer_id_to_secret_key.get(id).unwrap().clone())
            .collect();

        let mut state_machines = Vec::with_capacity(N);
        let mut message_producers = Vec::with_capacity(N);
        let mut broadcast_consumers = Vec::with_capacity(N);
        let mut tx_producers = Vec::with_capacity(N);
        let mut shutdown_signals = Vec::with_capacity(N);
        let mut temp_dirs = Vec::with_capacity(N);

        for (i, sk) in secret_keys.iter().enumerate() {
            let replica_id = peer_set.sorted_peer_ids[i];

            // Create message channel (for incoming consensus messages)
            let (msg_prod, msg_cons) = RingBuffer::new(1000);
            message_producers.push(msg_prod);

            // Create broadcast channel (for outgoing consensus messages)
            let (bc_prod, bc_cons) = RingBuffer::new(1000);
            broadcast_consumers.push(bc_cons);

            // Create transaction channel
            let (tx_prod, tx_cons) = RingBuffer::new(1000);
            tx_producers.push(tx_prod);

            // Create shutdown signal
            let shutdown = Arc::new(AtomicBool::new(false));
            shutdown_signals.push(shutdown.clone());

            // Create config
            let mut peer_strs = Vec::with_capacity(peer_set.sorted_peer_ids.len());
            for peer_id in &peer_set.sorted_peer_ids {
                let pk = peer_set.id_to_public_key.get(peer_id).unwrap();
                let mut buf = Vec::new();
                pk.0.serialize_compressed(&mut buf).unwrap();
                let peer_str = hex::encode(buf);
                peer_strs.push(peer_str);
            }
            let config = create_test_config(N, F, peer_strs);

            // Create leader manager
            let leader_manager = Box::new(RoundRobinLeaderManager::new(
                N,
                peer_set.sorted_peer_ids.clone(),
            ));

            // Create persistence storage
            let temp_dir = tempdir().unwrap();
            let path = temp_dir.path().join(format!("state_machine_{}", i));
            let storage = ConsensusStore::open(&path).unwrap();
            temp_dirs.push(temp_dir);

            // Create view manager
            let view_manager =
                ViewProgressManager::new(config, replica_id, storage, leader_manager).unwrap();

            // Create logger
            let logger = slog::Logger::root(slog::Discard, slog::o!());

            // Create state machine
            let state_machine = ConsensusStateMachine::new(
                view_manager,
                sk.clone(),
                msg_cons,
                bc_prod,
                tx_cons,
                Duration::from_millis(100),
                shutdown,
                logger,
            )
            .unwrap();

            state_machines.push(state_machine);
        }

        TestSetup {
            state_machines,
            peer_set,
            broadcast_consumers,
            shutdown_signals,
        }
    }

    #[test]
    fn test_minimmit_protocol_invariants() {
        // Test that key Minimmit invariants hold:
        // 1. M-notarization requires 2f+1 = 3 votes (with N=6, F=1)
        // 2. L-notarization requires n-f = 5 votes
        // 3. Nullification requires 2f+1 = 3 nullify messages
        assert_eq!(2 * F + 1, M_SIZE);
        assert_eq!(N - F, 5);
    }

    #[test]
    fn test_block_hash_computation_is_consistent() {
        let parent_hash = [0u8; blake3::OUT_LEN];
        let txs = vec![create_test_transaction(1), create_test_transaction(2)];
        let timestamp = 1234567890;
        let view = 1;

        let hash1 = compute_block_hash(parent_hash, &txs, timestamp, view);
        let hash2 = compute_block_hash(parent_hash, &txs, timestamp, view);

        // Same inputs should produce same hash
        assert_eq!(hash1, hash2);

        // Different timestamp should produce different hash
        let hash3 = compute_block_hash(parent_hash, &txs, timestamp + 1, view);
        assert_ne!(hash1, hash3);

        // Different view should produce different hash
        let hash4 = compute_block_hash(parent_hash, &txs, timestamp, view + 1);
        assert_ne!(hash1, hash4);
    }

    #[test]
    fn test_state_machine_creation() {
        // Test that we can create N state machines successfully
        let setup = create_test_setup();
        assert_eq!(setup.state_machines.len(), N);
        assert_eq!(setup.broadcast_consumers.len(), N);
        assert_eq!(setup.shutdown_signals.len(), N);
        assert_eq!(setup.peer_set.sorted_peer_ids.len(), N);
    }

    #[test]
    fn test_shutdown_stops_state_machine() {
        let mut setup = create_test_setup();
        let replica_idx = 0;

        // Shutdown should succeed
        let result = setup.state_machines[replica_idx].shutdown();
        assert!(result.is_ok());

        // Shutdown signal should be set
        assert!(setup.shutdown_signals[replica_idx].load(Ordering::Relaxed));
    }

    #[test]
    fn test_no_op_event_does_nothing() {
        let mut setup = create_test_setup();
        let replica_idx = 0;

        // NoOp event should succeed
        let event = ViewProgressEvent::NoOp;
        let result = setup.state_machines[replica_idx].handle_event(event);
        assert!(result.is_ok());

        // No messages should be broadcast
        let broadcast = setup.broadcast_consumers[replica_idx].pop();
        assert!(broadcast.is_err());
    }

    #[test]
    fn test_await_event_does_nothing() {
        let mut setup = create_test_setup();
        let replica_idx = 0;

        // Await event should succeed
        let event = ViewProgressEvent::Await;
        let result = setup.state_machines[replica_idx].handle_event(event);
        assert!(result.is_ok());

        // No messages should be broadcast
        let broadcast = setup.broadcast_consumers[replica_idx].pop();
        assert!(broadcast.is_err());
    }

    #[test]
    fn test_transaction_creation_has_valid_signature() {
        let tx = create_test_transaction(1);
        // Transaction should have valid signature
        assert!(tx.verify());

        // Different nonces should produce different transactions
        let tx2 = create_test_transaction(2);
        assert_ne!(tx.tx_hash, tx2.tx_hash);
    }

    #[test]
    fn test_peer_setup_creates_correct_number_of_peers() {
        let (peer_set, peer_id_to_secret_key) = create_test_peer_setup(N);

        assert_eq!(peer_set.sorted_peer_ids.len(), N);
        assert_eq!(peer_id_to_secret_key.len(), N);

        // All peer IDs should have corresponding secret keys
        for peer_id in &peer_set.sorted_peer_ids {
            assert!(peer_id_to_secret_key.contains_key(peer_id));
        }
    }

    #[test]
    fn test_consensus_config_creation() {
        let peers = vec!["peer1".to_string(), "peer2".to_string()];
        let config = create_test_config(N, F, peers.clone());

        assert_eq!(config.n, N);
        assert_eq!(config.f, F);
        assert_eq!(config.peers, peers);
        assert_eq!(config.view_timeout, Duration::from_secs(10));
        assert!(matches!(config.network, Network::Local));
        assert!(matches!(
            config.leader_manager,
            LeaderSelectionStrategy::RoundRobin
        ));
    }

    #[test]
    fn test_broadcast_channels_are_independent() {
        let mut setup = create_test_setup();

        // Each state machine should have its own broadcast channel
        // Verify we can access all of them
        for i in 0..N {
            assert!(setup.broadcast_consumers[i].pop().is_err()); // All empty initially
        }
    }
}
